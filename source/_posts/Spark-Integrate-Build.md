---
title: å¤§æ•°æ®ä¹‹è·¯ Spark ç¯å¢ƒæ­å»º
date: 2017-11-23 14:19:37
description: è®°å½•è¿™ç¯‡ Spark ç¯å¢ƒæ­å»ºä¸ä¹‹å‰é‚£ç¯‡ Hadoop ç¯å¢ƒæ­å»ºç›¸å‘¼åº”
categories: [å¤§æ•°æ®ç¯‡]
tags: [Spark]
---

<!-- more -->
## å‡†å¤‡å·¥ä½œ
### é¦–å…ˆ
é¦–å…ˆè¦è¯´æ˜çš„æ˜¯,æœ¬ç¯‡æ–‡ç« ç”¨çš„  Spark çš„ç‰ˆæœ¬éƒ½æ˜¯ç›®å‰æœ€æ–°ç‰ˆ,ç›´æ¥åœ¨å®˜ç½‘ä¸Šä¸‹è½½å°±å¯ä»¥äº†,æœ‰æ³¨æ„çš„,ä¸‹é¢è¯¦ç»†è¯´
æœ‰äº›å‘½ä»¤å¯èƒ½å·²ç»ä¸é€‚åº”ä¹‹å‰çš„æ—§ç‰ˆæœ¬äº†,ä»¥æœ€æ–°çš„ç‰ˆçš„ä¸ºå‡†
ä»¥ä¸‹æ“ä½œå‘½ä»¤å‡æ˜¯åœ¨æœåŠ¡çš„æ ¹ç›®å½•ä¸‹,ä½¿ç”¨çš„æ˜¯ç›¸å¯¹ç›®å½•

### å½“å‰ç‰ˆæœ¬è¯´æ˜
- jdk 1.8.0
- Hadoop ç‰ˆæœ¬2.8.2
- æ“ä½œç³»ç»Ÿç‰ˆæœ¬ centos 7.2
- Spark 2.2.0

### é¦–å…ˆéœ€è¦åšçš„
å®‰è£… jdk ç¯å¢ƒ,å†æ­¤ä¸åšè¯¦ç»†å™è¿°äº†,éœ€è¦æ³¨æ„çš„æ˜¯ jdk çš„ç¯å¢ƒå˜é‡çš„é…ç½®
å®‰è£… Hadoop ç¯å¢ƒ,å¿…é¡»å®‰è£… Hadoop æ‰èƒ½ä½¿ç”¨ Sparkï¼Œä½†å¦‚æœä½¿ç”¨ Spark è¿‡ç¨‹ä¸­æ²¡ç”¨åˆ° HDFSï¼Œä¸å¯åŠ¨ Hadoop ä¹Ÿæ˜¯å¯ä»¥çš„

## å®‰è£… Spark
æ‰“å¼€å®˜ç½‘ä¸‹è½½çš„åœ°å€: http://spark.apache.org/downloads.html
éœ€è¦æ³¨æ„çš„æ˜¯,åœ¨é€‰æ‹©ä¸‹è½½åŒ…ç±»å‹ `Choose a package type` è¿™ä¸ªéœ€è¦æ ¹æ®å®‰è£…çš„ Hadoop çš„ç‰ˆæœ¬æ¥å®šçš„,æˆ–è€…ç›´æ¥é€‰æ‹©  `Pre-build with user-provided Apache Hadoop `
è¿™æ ·æˆ‘ä»¬å¯ä»¥è‡ªå·±é…ç½® Hadoop çš„ç‰ˆæœ¬

ä¸‹è½½å,è§£å‹

è¿›å…¥ confç›®å½•æ‹·è´ä¸€ä»½é…ç½®æ–‡ä»¶

``` bash
    cp ./conf/spark-env.sh.template ./conf/spark-env.sh
```

åŠ å…¥ç¯å¢ƒå˜é‡

``` bash
    export SPARK_DIST_CLASSPATH=$(/home/hadoop-2.8.2/bin/hadoop classpath)
```

æˆ‘ä»¬è¿è¡Œ 

``` bash
    # ./sbin/start-all.sh
```
Spark ä¾¿ä¼šè¿è¡Œèµ·æ¥,æŸ¥çœ‹åœ°å€ : http://localhost:8080  å¯æŸ¥çœ‹åˆ°é›†ç¾¤æƒ…å†µ


## è¿è¡Œ Spark ç¤ºä¾‹ç¨‹åº
æ­£å¦‚å‰é¢çš„ Hadoop ä¸€æ ·, Spark è‡ªå¸¦æœ‰å¾ˆå¤šç¤ºä¾‹ç¨‹åº,ç›®å½•åœ¨ ./example ä¸‹é¢,æœ‰ Java çš„ Python,Scala ,R è¯­è¨€çš„,
è¿™é‡Œæˆ‘ä»¬é€‰ä¸ªæœ€ç†Ÿæ‚‰çš„ Java ç‰ˆçš„æ¥è·‘ä¸‹

æˆ‘ä»¬æ‰¾åˆ° Java çš„ç›®å½•é‡Œä¹Ÿèƒ½çœ‹åˆ°é‡Œé¢æœ‰å¾ˆå¤šç¨‹åº,èƒ½çœ‹åˆ°æˆ‘ä»¬ç†Ÿæ‚‰çš„ wordcount

è¿™é‡Œæˆ‘ä»¬è·‘ä¸ª è®¡ç®—Ï€çš„å€¼

``` bash
    # ./bin/run-example SparkPi
```

è¿è¡Œåæ§åˆ¶å°æ‰“å°å¾ˆå¤šä¿¡æ¯,ä½†æ˜¯èƒ½çœ‹åˆ°è¿™ä¹ˆä¸€è¡Œ:

_**Pi is roughly 3.1432557162785812**_

è¿™å°±å¯ä»¥äº†

## RDD
>> RDD : Spark çš„åˆ†å¸ƒå¼çš„å…ƒç´ é›†åˆï¼ˆdistributed collection of itemsï¼‰ï¼Œç§°ä¸ºRDDï¼ˆResilient Distributed Datasetï¼Œå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ï¼Œå®ƒå¯è¢«åˆ†å‘åˆ°é›†ç¾¤å„ä¸ªèŠ‚ç‚¹ä¸Šï¼Œè¿›è¡Œå¹¶è¡Œæ“ä½œã€‚RDDs å¯ä»¥é€šè¿‡ Hadoop InputFormats åˆ›å»ºï¼ˆå¦‚ HDFSï¼‰ï¼Œæˆ–è€…ä»å…¶ä»– RDDs è½¬åŒ–è€Œæ¥

æˆ‘å°±ç®€å•çš„ç†è§£ä¸º ç±»æ¯” Hadoop çš„ MapReduce

RDDs æ”¯æŒä¸¤ç§ç±»å‹çš„æ“ä½œ

- actions: åœ¨æ•°æ®é›†ä¸Šè¿è¡Œè®¡ç®—åè¿”å›å€¼
- transformations: è½¬æ¢, ä»ç°æœ‰æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†

## Spark-Shell
Spark-shell æ”¯æŒ Scala å’Œ Python 2ä¸­è¯­è¨€,è¿™é‡Œæˆ‘ä»¬å°±ç”¨ Scala æ¥åš,å…³äº Scala çš„ä½¿ç”¨å’Œè¯­æ³•æˆ‘æ‰“ç®—æ–°å†™ä¸€ç¯‡æ–‡ç« æ¥è®°å½•ä¸‹,
åœ¨ä¹‹å‰æˆ‘ä¹Ÿå†™è¿‡ åœ¨ maven ä¸­é›†æˆä½¿ç”¨ Scala æ¥ç¼–ç¨‹,è¿™é‡Œæˆ‘å…ˆç”¨ä¸‹

æ‰§è¡Œ shell 

``` bash
    # ./bin/spark-shell
    
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    17/11/24 09:33:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    17/11/24 09:33:37 WARN util.Utils: Your hostname, JoyLinux resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
    17/11/24 09:33:37 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
    Spark context Web UI available at http://10.0.2.15:4040
    Spark context available as 'sc' (master = local[*], app id = local-1511487218050).
    Spark session available as 'spark'.
    Welcome to
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /___/ .__/\_,_/_/ /_/\_\   version 2.2.0
          /_/
    
    Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)
    Type in expressions to have them evaluated.
    Type :help for more information.
    
    scala>
```

æ¥æ‰§è¡Œä¸€ä¸ªæ–‡æœ¬ç»Ÿè®¡

``` bash
    scala> val textFile = sc.textFile("file:///home/hadoop-2.8.2/input/test.txt").count()
    
    textFile: Long = 4
```

é»˜è®¤è¯»å–çš„æ–‡ä»¶æ˜¯ Hadoop HDFS ä¸Šçš„,ä¸Šé¢çš„ç¤ºä¾‹æ˜¯ä»æœ¬åœ°æ–‡ä»¶è¯»å–

æ¥ä¸€ä¸ªä» HDFS ä¸Šè¯»å–çš„,åœ¨è¿™é‡Œæˆ‘ä»¬ä¹‹å‰åœ¨ HDFS ä¸Šä¼ äº†ä¸ª tets.txt çš„æ–‡æ¡£,åœ¨è¿™é‡Œå°±å¯ä»¥ç›´æ¥ä½¿ç”¨äº†

``` bash
    scala> val textFile = sc.textFile("test2.txt");textFile.count()
    
    textFile: org.apache.spark.rdd.RDD[String] = test2.txt MapPartitionsRDD[19] at textFile at <console>:26
    res7: Long = 4
```

å¯ä»¥çœ‹åˆ°ç»“æœæ˜¯ä¸€æ ·çš„

## Spark SQL å’Œ DataFrames
Spark SQL æ˜¯ Spark å†…åµŒçš„æ¨¡å—ï¼Œç”¨äºç»“æ„åŒ–æ•°æ®ã€‚åœ¨ Spark ç¨‹åºä¸­å¯ä»¥ä½¿ç”¨ SQL æŸ¥è¯¢è¯­å¥æˆ– DataFrame APIã€‚DataFrames å’Œ SQL æä¾›äº†é€šç”¨çš„æ–¹å¼æ¥è¿æ¥å¤šç§æ•°æ®æºï¼Œæ”¯æŒ Hiveã€Avroã€Parquetã€ORCã€JSONã€å’Œ JDBCï¼Œå¹¶ä¸”å¯ä»¥åœ¨å¤šç§æ•°æ®æºä¹‹é—´æ‰§è¡Œ join æ“ä½œã€‚

ä¸‹é¢ä»åœ¨ Spark shell ä¸­æ¼”ç¤ºä¸€ä¸‹ Spark SQL çš„åŸºæœ¬æ“ä½œï¼Œè¯¥éƒ¨åˆ†å†…å®¹ä¸»è¦å‚è€ƒäº† Spark SQLã€DataFrames å’Œ Datasets æŒ‡å—ã€‚

Spark SQL çš„åŠŸèƒ½æ˜¯é€šè¿‡ SQLContext ç±»æ¥ä½¿ç”¨çš„ï¼Œè€Œåˆ›å»º SQLContext æ˜¯é€šè¿‡ SparkContext åˆ›å»ºçš„ã€‚

``` bash
    scala> var df = spark.read.json("file:///home/spark-2.2.0-bin-without-hadoop/examples/src/main/resources/employees.json")
    df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]
    
    scala> df.show()
    +-------+------+
    |   name|salary|
    +-------+------+
    |Michael|  3000|
    |   Andy|  4500|
    | Justin|  3500|
    |  Berta|  4000|
    +-------+------+
    
    
    scala>
```

å†æ¥æ‰§è¡Œ2æ¡æŸ¥è¯¢è¯­å¥
`df.select("name").show()`
`df.filter(df("salary")>=4000).show()`

``` bash
    scala> df.select("name").show()
    +-------+
    |   name|
    +-------+
    |Michael|
    |   Andy|
    | Justin|
    |  Berta|
    +-------+
    
    
    scala> df.filter(df("salary")>=4000).show()
    +-----+------+
    | name|salary|
    +-----+------+
    | Andy|  4500|
    |Berta|  4000|
    +-----+------+
```

æ‰§è¡Œä¸€æ¡ sql è¯­å¥è¯•è¯•

``` bash
    scala> df.registerTempTable("employees")
    warning: there was one deprecation warning; re-run with -deprecation for details
    
    scala> spark.sql("select * from employees").show()
    +-------+------+
    |   name|salary|
    +-------+------+
    |Michael|  3000|
    |   Andy|  4500|
    | Justin|  3500|
    |  Berta|  4000|
    +-------+------+
    
    
    scala> spark.sql("select * from employees where salary >= 4000").show()
    +-----+------+
    | name|salary|
    +-----+------+
    | Andy|  4500|
    |Berta|  4000|
    +-----+------+
```

å…¶å®è¿˜æœ‰å¾ˆå¤šåŠŸèƒ½å‘¢, http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame ,è¿™é‡Œå…ˆå†™2ä¸ªè¯•è¯•,åç»­å†ç»†èŠ‚å­¦ä¹ 

è¿™ç¯‡æ–‡ç« æš‚æ—¶å…ˆå†™åˆ°è¿™,è¿˜æœ‰åç»­çš„ Spark Streaming ,æƒ³å…ˆå­¦å­¦çœ‹æµå¼è®¡ç®—Storm,ä¹‹åå¯¹æ¯”ä¸‹çœ‹çœ‹å†™ä¸€ç¯‡æ–‡ç« 

æ¥ä¸‹æ¥,ç†Ÿæ‚‰ Scala è¯­æ³•å†™ä¸€ä¸ª JavaScala åº”ç”¨ç¨‹åºæ¥é€šè¿‡ SparkAPI å•ç‹¬éƒ¨ç½²ä¸€ä¸‹è¯•è¯•

### æ„Ÿå—
è¿™ç¯‡æ–‡ç« å†™ä¸‹æ¥ç­‰äºå°†å½“æ—¶æ­å»º Spark ç¯å¢ƒé‡å¤äº†ä¸€é, ä¹Ÿæ˜¯ä¸€éæ•²å‘½ä»¤,ä¸€éè®°å½•ä¸‹æ¥,æ¸©æ•…è€ŒçŸ¥æ–°,è‡ªå·±ä¹Ÿå­¦åˆ°ä¸å°‘ä¸œè¥¿,æ£’æ£’å“’ğŸ’¯